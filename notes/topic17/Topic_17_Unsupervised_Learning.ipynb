{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67fa9331",
   "metadata": {},
   "source": [
    "# Topic 17: Unsupervised Learning\n",
    "\n",
    "With unsupervised learning, there is no _training data_ for the machine learning, there are no 'correct examples' from which the algorithms can determine memberships or resultant values.  Instead, the goal of unsupervised learning is to find structure or relationships between the unlabeled samples.  With nothing else to go on, unsupervised learning looks for geometric or statistical relationships.\n",
    "\n",
    "Unsupervised learning typically seeks to answer one of the following four questions:\n",
    "\n",
    "* _Clustering_: The goal is to partition the data into groups.\n",
    "\n",
    "* _Dimensionality Reduction_: The goal is to reduce the dimensionality of the data, removing columns or combining columns.\n",
    "\n",
    "* _Outlier Detection_: Finding unusual events, such as malfunctions.\n",
    "\n",
    "* _Novelty Detection_: Identifying cases where changes occur in the data.\n",
    "\n",
    "The most common of these is clustering, which will be our focus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd4ec69",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Clustering seeks to group similar objects together, so that:\n",
    "\n",
    "* Examples within one cluster are similar, and\n",
    "\n",
    "* Examples in different clusters are different.\n",
    "\n",
    "This implies that we have some definition for what makes two examples _similar_ or _different_.\n",
    "\n",
    "One of the goals of clustering is to uncover _categories_ or _relationships_ between elements in a context where no labels had previously been given.\n",
    "\n",
    "There are two basic approaches to clustering, based on the types of information extracted from the elements:\n",
    "\n",
    "* In _similarity-based clustering_ an $ n * n $ _dissimilarity matrix_ or _distance matrix_ is determined, then the clustering algorithm uses this information to determine the clusters.\n",
    "\n",
    "* In _feature-based clustering_ an $ n * D $ _feature matrix_ is determined, then the clustering algorithm uses this information.\n",
    "\n",
    "One advantage of similarity-based clustering is that it is easy to enter a formula to compute a domain-specific similarity, so the clustering can be adapted to perform in various domains.\n",
    "\n",
    "One advantage of feature-based clustering is that it is less susceptible to noise.\n",
    "\n",
    "Consequently, there are a number of questions to ask when you are planning to perform a clustering analysis, and there is not always a single or optimal answer.\n",
    "\n",
    "* What is the natural grouping among the objects?  How would you define the \"distance\" or \"similarity\" between objects?\n",
    "\n",
    "* How can we group samples?  What are the best algorithms?  Are they efficient?\n",
    "\n",
    "* How many clusters should we look for in the data?  Do we need to specify this number at the start?  Do we allow very large or very small clusters?  Do we allow many or few clusters?\n",
    "\n",
    "* What constitutes a good grouping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa4f453",
   "metadata": {},
   "source": [
    "## Similarity and Distance\n",
    "\n",
    "The easiest measures of distance are geometrically based.  For instance:\n",
    "\n",
    "* _Manhattan distance_: For each feature of the two elements, compute the absolute value of the difference between the two values, then sum all of these differences.\n",
    "\n",
    "* _Euclidean distance_: For each feature of the two elements, square the difference between the two values, then sum all of these squares, then take the square root of that sum.\n",
    "\n",
    "* _Minkowski distance_: This is a generalization of the two previous equations.  For this algoritm, a value _p_ is chosen.  Then for each feature of the two elements, determine the absolute value of the difference between the two values, raising this to the _p_ power.  Sum all of those results, then take the *p*th root of that sum.\n",
    "\n",
    "If any features of the data are categorical, some method should be provided to compute a measure of \"distance\" between any pair of categories.  A simple method would be that identical categories have a distance of 0, and different categories have a distance of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567610df",
   "metadata": {},
   "source": [
    "## What Constitutes Good Clustering?\n",
    "\n",
    "If the data were labeled, we would be able to determine the quality of the clustering simply by comparing the prediction with the actual, groundtruth values, such as we did for Supervised Learning.  With Unsupervised Learning, we don't have these labels.  Instead, we rely on other procedures to assess the quality.  These procedures can themselves be grouped into two clusters:\n",
    "\n",
    "* Procedures to compare clustering techniques.  This is fairly indirect, we are simply asking, \"How well do the clusters from this technique match the clusters from that technique?\"  Interesting, but this doesn't really say \"how good\" either technique is!\n",
    "\n",
    "* Procedures that check on specific properties of the clustering, for example, \"compactness\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fa297a",
   "metadata": {},
   "source": [
    "### Rand Index\n",
    "\n",
    "The _Rand index_ or _Rand measure_ is one of the best-known methods for comparing the results of clustering.  This method evaluates the similary between two results of data clustering.\n",
    "\n",
    "Given a set of _n_ elements $ S = { o_1,...,o_n } $, we can compare two partitions of _S_:\n",
    "* $ X = {X_1,...,X_r} $, a partition of _S_ into _r_ subsets; and\n",
    "* $ Y = {Y_1,...,Y_s} $, a partition of _S_ into _s_ subsets.\n",
    "\n",
    "To do this, we compute the following four quantities:\n",
    "\n",
    "* _a_, the number of pairs of elements in _S_ that are in the same subset in both _X_ and _Y_;\n",
    "* _b_, the number of pairs of elements in _S_ that are in different subsets in both _X_ and _Y_;\n",
    "* _c_, the number of pairs of elements in _S_ that are in the same subset in _X_ but in different subsets in _Y_; and\n",
    "* _d_, the number of pairs of elements in _S_ that are in different subsets in _X_ but are in the same subset in _Y_.\n",
    "\n",
    "The Rand index, _R_, is defined as follows:\n",
    "\n",
    "$$ R = \\frac{a + b}{a + b + c + d} $$\n",
    "\n",
    "_R_ has a value that is between 0 and 1.\n",
    "\n",
    "Unfortunately, the Rand index is not deterministic.  Because the labeling of the clusters is arbitrary (determined solely by the clustering algorithms involved), with a different labeling, the Rand index might be different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05a359d",
   "metadata": {},
   "source": [
    "### Homogeneity Score\n",
    "\n",
    "One method to compare two clusters is the _homogeneity_ criterion.  We call the first clustering the _original_ and the second clustering the _predicted_.\n",
    "\n",
    "* A cluster in the predicted clustering satisfies homogeneity if all of its elements are in the same cluster in the original clustering.\n",
    "\n",
    "* If all of the clusters in the predicted clustering satisfy homogeneity, then the homogeneity score for the clustering will be 1.0.\n",
    "\n",
    "* If none of the clusters in the predicted clustering satisfy homogeneity, then the homogeneity score for the clustering will be 0.0.\n",
    "\n",
    "Here is the actual formula for the homogeneity score:\n",
    "\n",
    "$$ h = \\left[ \\begin{array}{ll}\n",
    "1 & \\mbox{if $H(C,K)$ = 0} \\\\\n",
    "1 - \\frac{H(C,K)}{H(C)} & \\mbox{else}\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    "$$ H(C,K) = - \\sum_{k=1}^{|K|} \\sum_{c=1}^{|C|} \\frac{a_{ck}}{N} \\log \\frac{a_{ck}}{\\sum_{c=1}^{|C|} a_{ck}} $$\n",
    "\n",
    "$$ H(C) = - \\sum_{c=1}{|C|} \\frac{\\sum_{k=1}^{|K|} a_{ck}}{n} \\log \\frac{\\sum_{k=1}^{|K|} a_{ck}}{n} $$\n",
    "\n",
    "You may ask, \"What does that formula mean?\"\n",
    "\n",
    "No one really knows.  So let's look at some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3434fb26",
   "metadata": {},
   "source": [
    "| 0 | 0 | 1 | 1 |\n",
    "|---|---|---|---|\n",
    "| 0 | 0 | 0 | 0 |\n",
    "\n",
    "The above diagram represents four samples.  The top row lists the original clustering, and the bottom row lists the predicted clustering.  In the original custering, there were two clusters: the first two samples were in the '0' cluster and the last two samples were in the '1' cluster.  In the predicted clustering, all of the samples were in the same cluster, cluster '0'.  The predicted cluster did _not_ satisfy homogeneity since some of the samples were from cluster '0' while others were from cluster '1'.  The score for this clustering was __0__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e94eb4",
   "metadata": {},
   "source": [
    "| 0 | 0 | 1 | 1 |\n",
    "|---|---|---|---|\n",
    "| 1 | 1 | 0 | 0 |\n",
    "\n",
    "For the above diagram, the original clustering had two clusters, and the predicted clustering had two clusters.  The two samples in the predicted '1' cluster both came from the same original cluster, and the two samples in the predicted '0' cluster both came from another original cluster.  Yes, the labeling of the clusters changed, but the labeling is just arbitrary numbering assigned by the clustering algorithm, the choice of numbers has no significance.  The score for this custering was __1__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ff1703",
   "metadata": {},
   "source": [
    "| 0 | 0 | 1 | 2 |\n",
    "|---|---|---|---|\n",
    "| 0 | 0 | 1 | 1 |\n",
    "\n",
    "In this case, the first predicted cluster contains two samples, and these both came from the same original cluster.  So this cluster does satisfy homogeneity.  The second predicted cluster, however, has two samples, but these came from different original clusters.  The second cluster does _not_ satisfy homogeneity.  Consequently, the overall homogeneity score some some value between 0 and 1.  A quick glance at the equation above, (or actually, a quick run through the program) shows that the score is __0.667__.  Its not obvious why this was the score, but this is what the formula produced.\n",
    "\n",
    "Before we go on, let's see how to run these scores through the program.  The above example was generated by this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba6db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import homogeneity_score\n",
    "print(\"%.3f\" % homogeneity_score([0, 1, 2, 2, 3, 3], [0, 0, 1, 1, 2, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c573b1c",
   "metadata": {},
   "source": [
    "| 0 | 0 | 0 | 0 |\n",
    "|---|---|---|---|\n",
    "| 0 | 1 | 1 | 0 |\n",
    "\n",
    "What about this example?  The predicted clustering contains two clusters.  The first, consisting of the first and fourth sample, both came from the same original cluster.  The second predicted cluster also had two samples, both of which came from _the same original cluster_.  It doesn't matter that both predicted clusters had elements from the _same_ original cluster, they both satisfy the homogeneity criterion, so the score for this clustering is __1__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36c4476",
   "metadata": {},
   "source": [
    "One more example:\n",
    "\n",
    "| 0 | 1 | 2 | 3 |\n",
    "|---|---|---|---|\n",
    "| 0 | 0 | 1 | 2 |\n",
    "\n",
    "The first cluster does not satisfy homogeneity, but the two remaining clusters do.  The score for this clustering is __0.75__.\n",
    "\n",
    "It is not obvious what the homogeneity number actually is, but we do know:\n",
    "\n",
    "* The score is 0 if there is no homogeneity is the clusters.\n",
    "* The score is 1 if every cluster satisfies homogeneity.\n",
    "* If some clusters do and some clusters don't satisfy homogeneity, the number is somewhere between 0 and 1.  The higher the homogeneity, the higher the score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7168ab",
   "metadata": {},
   "source": [
    "### Completeness Score\n",
    "\n",
    "Another score, closely related to the Homogeneity Score, is the _Completeness Score_.  A cluster satisfies the completeness criterion if all of the samples in this original cluster are members of the same predicted cluster.  This sounds a lot like the inverse of the Homogeneity Score, and indeed it is.  If the two clusterings are _A_ and _B_, then Homogeneity(_A_, _B_) = Completeness(_B_, _A_).  We just switch which clustering is considered the 'original' and which is considered the 'prediction'.\n",
    "\n",
    "For those who are curious, here is the actual formula for the completeness score:\n",
    "\n",
    "$$ c = \\left[ \\begin{array}{ll}\n",
    "1 & \\mbox{if $H(K,C)$ = 0} \\\\\n",
    "1 - \\frac{H(K,C)}{H(K)} & \\mbox{else}\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$ H(K,C) = - \\sum_{c=1}^{|C|} \\sum_{k=1}^{|K|} \\frac{a_{ck}}{N} \\log \\frac{a_{ck}}{\\sum_{k=1}^{|K|} a_{ck}} $$\n",
    "\n",
    "$$ H(K) = - \\sum_{k=1}{|K|} \\frac{\\sum_{c=1}^{|C|} a_{ck}}{n} \\log \\frac{\\sum_{c=1}^{|C|} a_{ck}}{n} $$\n",
    "\n",
    "Sadly, but not surprisingly, this formula is just as confusing!\n",
    "\n",
    "We don't list any examples here.  To see examples of the Completeness Score, just scroll back to the Homogeneity Score, but turn the page upside-down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fcbb51",
   "metadata": {},
   "source": [
    "### V-Measure\n",
    "\n",
    "We started talking about algorithms that are used to compare two clusterings.  The first example was the Rand Index.\n",
    "\n",
    "We then did a short digression talking about Homogeneity Score and Completeness Score.  These give a partial score, one measuring the clusters of the prediction and the other measuring the clusters of the original.\n",
    "\n",
    "Where these two scores came about is that they are the two components used to compute the second algorithm comparison, the _V-Measure_.\n",
    "\n",
    "The V-Measure is the harmonic mean between the Homogeneity and Completeness Scores:\n",
    "\n",
    "$$ v = 2 * \\frac{homogeneity * completeness}{homogeneity + completeness} $$\n",
    "\n",
    "* This measure is not dependent upon the labeling, so that if the label numbers are consistently changed, the score will not change.  \n",
    "\n",
    "* In addition, the score remains the same if the original and predicted clusterings are swapped.\n",
    "\n",
    "* The V-Measure is bounded: 0.0 means an extremely bad clustering, and 1.0 means a perfectly good clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895b940d",
   "metadata": {},
   "source": [
    "### Silhouette Score\n",
    "\n",
    "An alternative to the former scores is to evaluate the final 'shape' of the clustering.  It is defined as a function of two values:\n",
    "\n",
    "* _a_, the intracluster distance of a sample in the dataset.  The intercluster distance for sample _j_ is the distance to the nearest other sample from the same cluster.\n",
    "\n",
    "* _b_, the nearest-cluster distance for each sample.  The nearest-cluster distance for sample _j_ is the distance to the nearest sample from another cluster.\n",
    "\n",
    "The Silhouette coefficient for sample _i_ is:\n",
    "\n",
    "$$ Silhouette(i) = \\frac{b - a}{max(a, b)} $$\n",
    "\n",
    "If _Silhouette(i)_ is close to 0, it means that the sample is on the boarder of its cluster and its two nearest neighbors are approximately equidistant, with one being in the cluster and the other being in another cluster.\n",
    "\n",
    "If _Silhouette(i)_ is negative, it means that this sample is closer to a sample in a neighboring cluster than to any other sample in it's cluster.\n",
    "\n",
    "The average of the Silhouette coefficients for the samples in a cluster defines the \"goodness\" of the cluster.  A value close to 1 indicates a compact cluster.\n",
    "\n",
    "The average of the Silhouette coefficients across all of the samples gives an idea of the quality of the clustering result.\n",
    "\n",
    "Note that this score is significantly different that the previous scores.  All of the other scores compared two clusterings, to see how similar they were.  The Silhouette score applies to a single clustering, and gives a measure of the quality of that clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e111eabb",
   "metadata": {},
   "source": [
    "## Clustering Techniques\n",
    "\n",
    "There are several clustering techniques, and there are a couple of categories describing these techniques.  First:\n",
    "\n",
    "* _Soft partition_ algorithms assign a probability of a sample belonging to a cluster.\n",
    "\n",
    "* _Hard partition_ algorithms assign a sample to a specific cluster.\n",
    "\n",
    "Second, the hard partition algorithms can be divided into two families of algorithms:\n",
    "\n",
    "* _Partitional algorithms_: These start with a random partition, then perform iterative improvements.\n",
    "\n",
    "* _Hierarchical algorithms_: These algorithms form partitions as _trees_.\n",
    "\n",
    "There are two approaches for constructing these hierarchical trees:\n",
    "\n",
    "* _Bottom-up_, where each sample starts in its own partition, but then low-level partitions are merged to form higher-level partions.  At some point this aggregation terminates (or else every sample would wind up in a single partition).\n",
    "\n",
    "* _Top-down_, where all the samples start in one partition, but then high-level partitions are split to form multiple lower-level partitions.  At some point this segregation terminates (or else every sample would wind up in its own partition)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d731aa",
   "metadata": {},
   "source": [
    "First, we load up the Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92f6719",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn import cluster, datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.datasets import make_blobs,make_moons\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "#plt.rc('font', family='times')\n",
    "plt.rc('xtick', labelsize=10) \n",
    "plt.rc('ytick', labelsize=10) \n",
    "plt.rc('font', size=12) \n",
    "plt.rc('figure', figsize = (12, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7e4ffa",
   "metadata": {},
   "source": [
    "## K-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3d9f94",
   "metadata": {},
   "source": [
    "The K-means algorithm is a hard partition algorithm that assigns each sample to a cluster.  The name of the algorithm means:\n",
    "\n",
    "* \"K\" - The algorithm is going to form _K_ clusters.\n",
    "\n",
    "* \"means\" - The _mean_ is the centroid or average.\n",
    "\n",
    "The algorithm works as follows:\n",
    "\n",
    "* Start by picking _K_ random initial 'centroids', the seeds of the clusters.\n",
    "\n",
    "* Place each sample into a cluster by finding the closest centroid, then joining that cluster.\n",
    "\n",
    "* For each group, compute a new centroid by averaging the locations of each sample.\n",
    "\n",
    "* Reevaluate where each sample should be placed: With the new centroids, where should each sample now be placed.  If every sample remains in the same group, terminate the algorithm.  If any sample changes cluster, loop back to the previous step.\n",
    "\n",
    "Note that this algorithm is not guaranteed to find a global minimum, it may get stuck into a local minimum.  The usual approach to get around this limitation is to run the algorithm a number of times, with different starting random locations.  The best solution is retained as the solution to the problem.  The default number of iterations is 10.\n",
    "\n",
    "The following code generates a sample problem for this algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b9a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create some data\n",
    "MAXN=40\n",
    "X = np.concatenate([1.25*np.random.randn(MAXN,2), 5+1.5*np.random.randn(MAXN,2)]) \n",
    "X = np.concatenate([X,[8,3]+1.2*np.random.randn(MAXN,2)])\n",
    "\n",
    "#Just for visualization purposes, create the labels of the 3 distributions\n",
    "y = np.concatenate([np.ones((MAXN,1)),2*np.ones((MAXN,1))])\n",
    "y = np.concatenate([y,3*np.ones((MAXN,1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a02cb",
   "metadata": {},
   "source": [
    "In the following two plots, we see the data.  In the left plot, we have colored the points to show which cluster they were created within.  The right plot shows the data that the algorithm sees: Just the points, with no clue as to the coloring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c94d45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\n",
    "plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\n",
    "plt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n",
    "plt.title('Data as were generated')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X[:,0],X[:,1],color='r')\n",
    "plt.title('Data as the algorithm sees them')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d7fc3",
   "metadata": {},
   "source": [
    "One thing I noticed: In the overlap between the blue and green clusters, we just have points.  Remember, the points don't really have labels (in this case colors).  We just did that to illustrate what the program is doing.  We are asking the algorithm to somehow separate the green and blue dots.  Look at the two dots at roughly (5.9, 5.5).  These two points are very close together, but the 'correct' answer is that they are in different groups.\n",
    "\n",
    "The K-means implementation uses the Silhouette metric to evaluate the effectiveness, and this requires generating a large $ n \\times n $ matrix.  To make things more efficient, the code attempts to use the graphics accelerator on the computer.  But this doesn't work on a Mac."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa15d70",
   "metadata": {},
   "source": [
    "In the following we will do the actual clustering. There are techniques that can help us determine the number of clusters to use, but for now we will simply say that there should be 3 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f0a101",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3\n",
    "clf = cluster.KMeans(init='random', n_clusters=K, n_init=10, max_iter=300, random_state=12)\n",
    "clf.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4877f8",
   "metadata": {},
   "source": [
    "* \"init='random'\" tells the algorithm to randomize the initial locations of the centroids..\n",
    "* \"n_clusters\" sets the number of clusters.\n",
    "* \"n_init\" says to run the whole process 10 times.  The _K-means_ clusterizer can get stuck in local minima, so by running multiple times we try to avoid these less-optimal solutions.\n",
    "* \"max_iter\": Recall that _K-means_ iteratively refines the centroids of the clusters.  The process stops when the samples do not change clusters.  However, sometimes the process may iterate with very minimal changes, so this factor can be used to make sure the process terminates.\n",
    "* \"random_state\" initializes the random number generator, as was mentioned in other lectures.\n",
    "\n",
    "The *labels_* attribute of the clustering method indicates the final answer, showing for each sample which cluster it was assigned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07acb5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee0081e",
   "metadata": {},
   "source": [
    "Ideally, the first 40 points would all belong to one cluster, the next 40 in a second cluster, and the remaining points in the third cluster.  We see, however, there are a few points that are assigned to the wrong cluster.\n",
    "\n",
    "Let's look again at the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71b15b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\n",
    "plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\n",
    "plt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((6, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12062c81",
   "metadata": {},
   "source": [
    "We can see that the blue and green clusters overlap, so there are some green points deep within the blue territory, and a blue dot close to the center of the green.  Without the coloring information, how is the clustering program supposed to know the correct placement of these colors?  We cannot expect the program to get 100% accuracy!\n",
    "\n",
    "What we want to do next is to draw the territories for each of the clusters.  How do we do that?  The technique they use is this:\n",
    "\n",
    "* Build a _mesh grid_, an array of 200 x 200 points covering the area of our drawing.  \n",
    "* Take all of those grid points and run them through the trained clustering algorithm.  This will then indicate the cluster that would be assigned to each of the grid points.  \n",
    "* Use _implot_ to plot the mesh \"image\".  This plot will be drawn a little blurred, so the individual points merge.  \n",
    "* Draw the original data points over the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf01a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5,15, 200)\n",
    "XX, YY = np.meshgrid(x, x)\n",
    "sz = XX.shape\n",
    "data = np.c_[XX.ravel(), YY.ravel()]\n",
    "Z = clf.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726bf11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Z.reshape(sz), interpolation='bilinear', origin='lower', extent=(-5, 15, -5, 15), alpha=0.3, vmin=0, vmax=K-1)\n",
    "plt.title('Space partitions', size=14)\n",
    "plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\n",
    "plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\n",
    "plt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((6, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fcb856",
   "metadata": {},
   "source": [
    "The plot shows that the clusters were separated quite nicely, and we can also see that a few of the points are 'mis-categorized'.  Again, the program actually did the right thing, those 'incorrect' points are really inside the limits of the other cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33226af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = cluster.KMeans(n_clusters=K, random_state=0, n_init = 'auto')\n",
    "clf.fit(X)\n",
    "\n",
    "data=np.c_[XX.ravel(),YY.ravel()]\n",
    "Z=clf.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b4cf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Final result of K-means', size=14)\n",
    "\n",
    "plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\n",
    "plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\n",
    "plt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n",
    "\n",
    "plt.imshow(Z.reshape(sz), interpolation='bilinear', origin='lower', extent=(-5, 15, -5, 15), alpha=0.3, vmin=0, vmax=K-1)\n",
    "\n",
    "x = np.linspace(-5,15, 200)\n",
    "XX, YY = np.meshgrid(x, x)\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((6, 5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe00ccb",
   "metadata": {},
   "source": [
    "The K-means algorithm clusters the data by finding the optimal centroids for the clusters. It chooses these centroids by minimizing a criterion known as the inertia, which is the sum-of-squares distance from each point of a cluster to its centroid.\n",
    "\n",
    "This inertia is thought of as a measure of the compactness of the cluster, or as 'a measure of how internally coherent cluster are'.\n",
    "\n",
    "We can view some metrics of the clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bb2200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe56151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cluster(n_clusters, init, max_iter, n_init):\n",
    "  clf = cluster.KMeans(n_clusters=n_clusters, init=init, random_state=0, max_iter=max_iter, n_init=n_init)\n",
    "  clf.fit(X)\n",
    "  print('Final evaluation of the clustering:')\n",
    "  print(f'Inertia: {clf.inertia_:.2f}')\n",
    "  print(f'Adjusted_rand_score: {metrics.adjusted_rand_score(y.ravel(), clf.labels_):.2f}')\n",
    "  print(f'Homogeneity: {metrics.homogeneity_score(y.ravel(), clf.labels_):.2f}')\n",
    "  print(f'Completeness: {metrics.completeness_score(y.ravel(), clf.labels_):.2f}')\n",
    "  print(f'V_measure: {metrics.v_measure_score(y.ravel(), clf.labels_):.2f}')\n",
    "  print(f'Silhouette: {metrics.silhouette_score(X, clf.labels_, metric=\"euclidean\"):.2f}')\n",
    "  plt.title(f'K-means, {n_clusters} clusters, {max_iter} iterations, {n_init} n_init, {init}', size=14)\n",
    "  plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\n",
    "  plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\n",
    "  plt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n",
    "  x = np.linspace(-5,15, 200)\n",
    "  XX, YY = np.meshgrid(x, x)\n",
    "  data=np.c_[XX.ravel(),YY.ravel()]\n",
    "  Z=clf.predict(data)\n",
    "  plt.imshow(Z.reshape(sz), cmap='hsv', interpolation='bilinear', origin='lower', extent=(-5, 15, -5, 15), alpha=0.6, vmin=0, vmax=n_clusters-1)\n",
    "  fig = plt.gcf()\n",
    "  fig.set_size_inches((6, 5))\n",
    "  plt.show()\n",
    "\n",
    "test_cluster(3, 'k-means++', 300, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aa004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cluster(3, 'random', 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68066c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cluster(3, 'k-means++', 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bc8f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cluster(8, 'k-means++', 300, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7831e6",
   "metadata": {},
   "source": [
    "Here are some issues with k-means:\n",
    "\n",
    "* Inertia makes the assumption that the clusters are convex and isotropic, which is not always the case.  It responds poorly to elongated clusters, or manifolds with irregular shapes.\n",
    "\n",
    "* Given enough time, K-means will always converge, although it may be converging on a local minima.\n",
    "\n",
    "* The algorithm requires the number of clusters to be specified.\n",
    "\n",
    "* It scales well to a large number of samples and has been used across a large range of application areas in many different fields.\n",
    "\n",
    "* The computation ios often done several times (with an n_init > 1), with different initializations of the centroids.  This is to help alleviate the local minima issue.\n",
    "\n",
    "* The _k-means++_ initialization scheme initializes the centroids to be generally distant from each other, leading to better results than the _random_ initialization scheme.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caabae2d",
   "metadata": {},
   "source": [
    "## Spectral Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b39fd7a",
   "metadata": {},
   "source": [
    "With the K-means cluster algorithm, clusters are formed based on _compactness_.  For Spectral clustering, clusters are formed based on _connectivity_.\n",
    "\n",
    "The algorithm needs an algorithm for determing a \"distance\" between two samples.  In many case this \"distance\" is in fact the distance between the samples.  We then want samples from the same cluster to be somewhat close together, but samples from different clusters to be further apart.\n",
    "\n",
    "That is a bit of a simplification, however.  Suppose we have a sequence of samples, $ p_0, p_1,..., p_n $, where $ p_0 $ is very close to $ p_1 $, and $ p_1 $ is close to $ p_2 $, and so on.  So each of these neighboring pairs are close together, and hence should be in the same cluster.  However, $ p_0 $ might be quite distance from $ p_n $, and there may be samples from other clusters that are closer to $ p_0 $ than $p_n$ is.\n",
    "\n",
    "For example, Spectral Clustering might compute a _similarity matrix_ giving a measure of the similarity between any two samples in the dataset.  The algorithm then merges or clusters the samples that are the closest together.\n",
    "\n",
    "The expectation is that two points in the same cluster are close together.  However, two distant points might be in the same cluster if there is a 'path' between them (of other samples), such that the distance between subsequent pairs of samples on the path are small.\n",
    "\n",
    "Scikit-learn has some tools in its library to generate datasets for use in illustrating these algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6a3ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs, make_moons\n",
    "\n",
    "X, labels_true = make_blobs(n_samples = 1000, centers = 3, cluster_std=[1.7,1.7,1.7])\n",
    "[Xmoons, ymoons] = make_moons(n_samples=300, noise=0.05)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c='r', marker='o', s=20)\n",
    "plt.axis('equal')\n",
    "plt.title(\"Compact Clusters\", size=14)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(Xmoons[:,0], Xmoons[:,1], c='r', marker='o', s=20)\n",
    "plt.axis('equal')\n",
    "plt.title(\"Connectivity-Based Clusters\", size=14)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((11,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d506ddb0",
   "metadata": {},
   "source": [
    "We can apply the Spectral clustering on the two moons problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc6a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics import euclidean_distances\n",
    "colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])\n",
    "colors = np.hstack([colors] * 20)\n",
    "\n",
    "# compute the distances\n",
    "distances = euclidean_distances(Xmoons)\n",
    "\n",
    "spectral = cluster.SpectralClustering(n_clusters=2, affinity=\"nearest_neighbors\")\n",
    "spectral.fit(Xmoons)\n",
    "y_pred = spectral.labels_.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351e1fb4",
   "metadata": {},
   "source": [
    "We can now visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac81f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.scatter(Xmoons[:, 0], Xmoons[:, 1], c='r', marker='o', s=20)\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(Xmoons[y_pred==0, 0], Xmoons[y_pred==0, 1], c='b', marker='o', s=20)\n",
    "plt.scatter(Xmoons[y_pred==1, 0], Xmoons[y_pred==1, 1], c='y', marker='o', s=20)\n",
    "plt.axis('equal')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((11, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b075a0",
   "metadata": {},
   "source": [
    "In the book, this showed a perfect clustering of the points. Sometimes when I run it, one of the horns of a moon is in the wrong cluster. However, next time this is run, it might be correct!\n",
    "\n",
    "We can compare the results to K-means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47178283",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.scatter(Xmoons[:, 0], Xmoons[:, 1], c='r', marker='o', s=20)\n",
    "plt.axis('equal')\n",
    "\n",
    "clf = cluster.KMeans(n_clusters=2, init='k-means++', n_init='auto')\n",
    "clf.fit(Xmoons)\n",
    "y_pred = clf.predict(Xmoons)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(Xmoons[y_pred==0, 0], Xmoons[y_pred==0, 1], c='b', marker='o', s=20)\n",
    "plt.scatter(Xmoons[y_pred==1, 0], Xmoons[y_pred==1, 1], c='y', marker='o', s=20)\n",
    "plt.axis('equal')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((11, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc45a6c5",
   "metadata": {},
   "source": [
    "As we can see, the K-means looks for spherical clusters, so it has a different result than the connection-based Spectral clusterer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdfc33b",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07a179a",
   "metadata": {},
   "source": [
    "There are several hierarchical clustering algorithms.  Some start with all of the samples in one cluster, then repeatedly split clusters into smaller clusters, and when this is taken to extremes, each sample is in its own cluster.  Others start with each of the samples in its own cluster, then repeatedly join two clusters into one, and when this is taken to extremes, only a single cluster remains containing all of the samples.  The key in both of these instances is to stop the splitting or merging when there is the right number of clusters, _K_.\n",
    "\n",
    "We will be looking at _agglomerative clustering_, which is a bottom-up algorithm.\n",
    "\n",
    "At each step in the merging, we want to find the two \"closest\" clusters, then merge them into one.  But how do we define the \"closeness\" or \"similarity\" of two clusters?\n",
    "\n",
    "* _Maximum_ or _complete_ linkage minimizes the maximum distance between samples of a pair of clusters.  Based on the similariy of the two least similar members of the clusters, this clustering tends to give tight spherical clusters.\n",
    "\n",
    "* _Average_ linkage averages similarity between members, it minimizes the average of the distances between all samples in a pair of clusters.\n",
    "\n",
    "* _Single_ linkage uses the similarity between the two most similar samples of the clusters.\n",
    "\n",
    "* _Ward_ linkage minimizes the sum of squared differences within all clusters.  It is thus a variance-minimizing approach and in this sense is similar to the K-means function.\n",
    "\n",
    "The following example generates a number of samples.  The _y1_ array indicates which 'cluster' in which the sample were generated.  In the following plots, the samples are drawn with a character that indicates the original cluster and in a color that indicates the cluster as chosen by the algorithm.  _Note: in real life data, the points do not have an 'original' cluster!_\n",
    "\n",
    "### Funny Note\n",
    "\n",
    "I ran the code as written in the book.  The resulting pictures looked **identical** to me!  I looked in the later version of the book, and again, they were identical.  I looked at the code: they ran the clustering algorithm _but they never used the output of the clustering_, so they were indeed drawing the identical image.\n",
    "\n",
    "So I looked on-line for another example, saw how they found the output of the clustering, then applied that to this code.  And now these pictures look different!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0369f3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXN1 = 500\n",
    "MAXN2 = 400\n",
    "MAXN3 = 300\n",
    "X1 = np.concatenate ([2.25*np.random.randn(MAXN1,2),4+1.7* np.random.randn (MAXN2 ,2)])\n",
    "X1 = np.concatenate ([X1,[8,3]+1.9* np.random.randn(MAXN3 ,2)])\n",
    "y1 = np.concatenate ([ np.ones ((MAXN1,1)),2* np.ones((MAXN2,1))])\n",
    "y1 = np.concatenate ([y1,3* np.ones((MAXN3,1))]).ravel()\n",
    "y1 = np.int_(y1)\n",
    "labels_y1=['+','*','o']\n",
    "colors=['r','g', 'b']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d66942",
   "metadata": {},
   "source": [
    "That just generated the data.  The following will plot the data.  It takes a little bit of time for each plot to form, and they are drawn one at a time, so give this a minute or two to complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce595d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for linkage in ('ward', 'complete', 'average', 'single'):\n",
    "    clustering = AgglomerativeClustering(linkage=linkage,n_clusters =3)\n",
    "    clustering.fit(X1)\n",
    "    yp = clustering.labels_.astype(int)\n",
    "    x_min , x_max = np.min (X1, axis =0) , np.max (X1,axis =0)\n",
    "    X1 = (X1 - x_min ) / ( x_max - x_min )\n",
    "    fig = plt.figure ()\n",
    "    fig.set_size_inches((5,5))\n",
    "    for i in range (X1.shape [0]) :\n",
    "        plt.text(X1[i,0],X1[i,1],labels_y1[y1[i]-1],color=colors[yp[i]-1])\n",
    "    plt.title (\"%s linkage\" % linkage,size =20)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acc9571",
   "metadata": {},
   "source": [
    "In many cases, the agglomerative clustering can generate very unbalanced cluster sizes.  Average linking is worst at this.  Ward linkage tries to form compact clusters.\n",
    "\n",
    "For non-geometric problems, Average linkage actually works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d95c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
